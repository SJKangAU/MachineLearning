{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a4cf4d",
   "metadata": {},
   "source": [
    "# Toxicity Classification in Online Comments\n",
    "\n",
    "In the age of social media, it's difficult for developers to monitor every single remark, blog. People utilise this platform to share and learn information at times. However, some people use toxic remarks that are insulting, vulgar, and can cause some people to abandon the debate. As a result, many individuals stop expressing themselves and stop searching out alternative viewpoints. Because platforms fail to properly facilitate dialogues, several communities limit or totally disable user comments.\n",
    "\n",
    "In this notebook several machine learning algorithms are applied to the jicksaw text data and their performance in terms of Accuracy is compared. Models considered are naive-Bayes, MLP Classifier and Random Forest and their performance is evaluated using accuracy metric. An aspect considered here is the inbalance of class frequencies. While for most algorithms in this notebook a feature matrix based on weights of particular words of the comment-text is employed (TF-idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245635d1",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n",
    " The data is downloaded from Kaggle and this <a href=\"https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data\">link</a> will take you to the page on kaggle where you can download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf721c",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1824e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprettytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     20\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.random import RandomState\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8468d",
   "metadata": {},
   "source": [
    "### Import data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emb_data = pd.read_csv(\"dataset/train_embedding.csv\")\n",
    "train_raw_data = pd.read_csv(\"dataset/train_raw.csv\")\n",
    "train_tfidf_data = pd.read_csv(\"dataset/train_tfidf.csv\")\n",
    "# dev_emb_data = pd.read_csv(\"dataset/dev_embedding.csv\")\n",
    "# dev_raw_data = pd.read_csv(\"dataset/dev_raw.csv\")\n",
    "dev_tfidf_data = pd.read_csv(\"dataset/dev_tfidf.csv\")\n",
    "# test_emb_data = pd.read_csv(\"dataset/test_embedding.csv\")\n",
    "# test_raw_data = pd.read_csv(\"dataset/test_raw.csv\")\n",
    "test_tfidf_data = pd.read_csv(\"dataset/test_tfidf.csv\")\n",
    "# unlabeled_emb_data = pd.read_csv(\"dataset/unlabeled_embedding.csv\")\n",
    "# unlabeled_raw_data = pd.read_csv(\"dataset/unlabeled_raw.csv\")\n",
    "# unlabeled_tfidf_data = pd.read_csv(\"dataset/unlabeled_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a645b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Using only raw training data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c20201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing first few rows of raw data\n",
    "train_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing information of raw training data\n",
    "train_raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying shape of raw training data\n",
    "train_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b56aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical info of dataset\n",
    "train_raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014570a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_check = train_raw_data[train_raw_data['Comment'].isnull()]\n",
    "len(cmt_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2c62",
   "metadata": {},
   "source": [
    "## Basic Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1f51d",
   "metadata": {},
   "source": [
    "**Distribution for words in comment text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data['num_words'] = [len(sent.split()) for sent in train_raw_data['Comment']]\n",
    "train_raw_data[['Comment', 'num_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafae232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum and minimum number of words per sentencs\n",
    "max(train_raw_data['num_words']), min(train_raw_data['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data['toxic'] = np.where(train_raw_data['Toxicity'] >= .5, 'Toxic', 'Non-Toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e985dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,7))\n",
    "sns.histplot(train_raw_data[train_raw_data['toxic'] == 'Toxic']['num_words'], kde= True, ax=ax[0], color= 'r')\n",
    "sns.histplot(train_raw_data[train_raw_data['toxic'] == 'Non-Toxic']['num_words'], kde= True, ax=ax[1], color= 'b')\n",
    "ax[0].set_title(\"Toxic Distribution\")\n",
    "ax[1].set_title(\"Non Toxic Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d32f1",
   "metadata": {},
   "source": [
    "**Distribuition of Toxicity of comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45968ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9954b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_raw_data['toxic'])\n",
    "plt.title('Distribuition of Toxicity of comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a list of all the identities tagged in this dataset.\n",
    "identities = ['Asian', 'Atheist', 'Bisexual', 'Black', 'Buddhist',\n",
    "       'Christian', 'Female', 'Heterosexual', 'Hindu',\n",
    "       'Homosexual gay or lesbian', 'Intellectual or learning disability',\n",
    "       'Jewish', 'Latino', 'Male', 'Muslim', 'Other disability',\n",
    "       'Other gender', 'Other race or ethnicity', 'Other religion',\n",
    "       'Other sexual orientation', 'Physical disability',\n",
    "       'Psychiatric or mental illness', 'Transgender', 'White']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the dataframe with identities tagged\n",
    "train_labeled_df = train_raw_data.loc[:, ['Toxicity'] + identities ].dropna()\n",
    "\n",
    "# lets define toxicity as a comment with a score being equal or .5\n",
    "toxic_df = train_labeled_df[train_labeled_df['Toxicity'] >= .5][identities]\n",
    "non_toxic_df = train_labeled_df[train_labeled_df['Toxicity'] < .5][identities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\n",
    "toxic_count = toxic_df.apply(lambda x : x > 0 ).sum()\n",
    "non_toxic_count = non_toxic_df.apply(lambda x : x > 0 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows= 2,ncols= 1,figsize=(15,15))\n",
    "\n",
    "sns.barplot(x= toxic_count, y= identities, ax=ax[0])\n",
    "sns.barplot(x= non_toxic_count, y= identities, ax=ax[1])\n",
    "\n",
    "ax[0].set_title(\"Toxic Distribution\")\n",
    "ax[1].set_title(\"Non Toxic Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b5a06",
   "metadata": {},
   "source": [
    "## Preprocessing data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f11b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can concat the two series together to get a toxic count vs non toxic count for each identity\n",
    "toxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\n",
    "\n",
    "toxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n",
    "# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\n",
    "toxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing first few rows of tfidf training data\n",
    "train_tfidf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd76c4",
   "metadata": {},
   "source": [
    "### Subsetting the dataset for only TFIDF Comment feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_tfidf_data.iloc[:,26:]\n",
    "y_train = train_tfidf_data['Toxicity']\n",
    "\n",
    "X_test = dev_tfidf_data.iloc[:,26:]\n",
    "y_test = dev_tfidf_data['Toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d94e3",
   "metadata": {},
   "source": [
    "## Modeling for Classification of Toxicity in Online Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6e305f",
   "metadata": {},
   "source": [
    "## 1. Random Forest Classsifier (Baseline)\n",
    "\n",
    "Training Random Forest Classifier with only TFIDF comment feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier\n",
    "clf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    " \n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ea496",
   "metadata": {},
   "source": [
    "**ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_RF = clf.predict_proba(X_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_RF)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba_RF)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798513a",
   "metadata": {},
   "source": [
    "## 2. MLP Classifier\n",
    "\n",
    "Training MLP Classifier with only TFIDF comment feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468faea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "clf_MLP = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "\n",
    "# predict \n",
    "y_pred_MLP=clf_MLP.predict(X_test)\n",
    "\n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred_MLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010bef3",
   "metadata": {},
   "source": [
    "**ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e39561",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf_MLP.predict_proba(X_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fc8d0",
   "metadata": {},
   "source": [
    "## 3. Multinomial Naive Bayes\n",
    "\n",
    "Training Multinomial Naive Bayes with only TFIDF comment feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(X_train, y_train)\n",
    "\n",
    "y_pred_NB = clf_NB.predict(X_test)\n",
    "\n",
    "\n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d1104",
   "metadata": {},
   "source": [
    "**ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d10537",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_NB = clf_NB.predict_proba(X_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_NB)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba_NB)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c70e6",
   "metadata": {},
   "source": [
    "### Training over all identity categories\n",
    "\n",
    "Training Baseline model with whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193545f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_f = train_tfidf_data['Toxicity']\n",
    "X_train_f = train_tfidf_data.drop([\"ID\",\"Toxicity\"], axis=1)\n",
    "\n",
    "y_test_f = dev_tfidf_data['Toxicity']\n",
    "X_test_f = dev_tfidf_data.drop([\"ID\",\"Toxicity\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8c3b4",
   "metadata": {},
   "source": [
    "**Training Baseline Model with complete dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier\n",
    "clf_f = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf_f.fit(X_train_f, y_train_f)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_RF_f = clf_f.predict(X_test_f)\n",
    " \n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test_f, y_pred_RF_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = {\"index\": test_tfidf_data[\"ID\"], \"Toxicity\": y_pred_lr}\n",
    "submission = pd.DataFrame(sub_data)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f05d1",
   "metadata": {},
   "source": [
    "**Feature Scores** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the feature scores\n",
    "\n",
    "feature_scores = pd.Series(clf_f.feature_importances_, index=X_train_f.columns).sort_values(ascending=False)\n",
    "\n",
    "display(feature_scores)\n",
    "plt.figure(figsize=(30,24))\n",
    "feat_importances = pd.Series(clf_f.feature_importances_, index=X_train_f.columns).sort_values(ascending=False)\n",
    "feat_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa4b7c",
   "metadata": {},
   "source": [
    "### Subsetting the dataset for TFIDF Comment feature values + Religion identities (Asian, Atheist,Buddhist, Christian, Hindu, Jewish, Muslim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa10fdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tfidf_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_sub0 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tfidf_data\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m26\u001b[39m:]\n\u001b[0;32m      2\u001b[0m X_train_sub0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsian\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_tfidf_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsian\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m X_train_sub0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtheist\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_tfidf_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtheist\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_tfidf_data' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_sub0 = train_tfidf_data.iloc[:,26:]\n",
    "X_train_sub0[\"Asian\"] = train_tfidf_data[\"Asian\"]\n",
    "X_train_sub0[\"Atheist\"] = train_tfidf_data[\"Atheist\"]\n",
    "X_train_sub0[\"Buddhist\"] = train_tfidf_data[\"Buddhist\"]\n",
    "X_train_sub0[\"Christian\"] = train_tfidf_data[\"Christian\"]\n",
    "X_train_sub0[\"Hindu\"] = train_tfidf_data[\"Hindu\"]\n",
    "X_train_sub0[\"Jewish\"] = train_tfidf_data[\"Jewish\"]\n",
    "X_train_sub0[\"Muslim\"] = train_tfidf_data[\"Muslim\"]\n",
    "# ['Asian', 'Atheist','Buddhist', 'Christian', 'Hindu', 'Jewish', 'Muslim']\n",
    "y_train_sub0 = train_tfidf_data['Toxicity']\n",
    "\n",
    "X_test_sub0 = dev_tfidf_data.iloc[:,26:]\n",
    "X_test_sub0[\"Asian\"] = dev_tfidf_data[\"Asian\"]\n",
    "X_test_sub0[\"Atheist\"] = dev_tfidf_data[\"Atheist\"]\n",
    "X_test_sub0[\"Buddhist\"] = dev_tfidf_data[\"Buddhist\"]\n",
    "X_test_sub0[\"Christian\"] = dev_tfidf_data[\"Christian\"]\n",
    "X_test_sub0[\"Hindu\"] = dev_tfidf_data[\"Hindu\"]\n",
    "X_test_sub0[\"Jewish\"] = dev_tfidf_data[\"Jewish\"]\n",
    "X_test_sub0[\"Muslim\"] = dev_tfidf_data[\"Muslim\"]\n",
    "y_test_sub0 = dev_tfidf_data['Toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c03c6",
   "metadata": {},
   "source": [
    "**Training Baseline Model with a subset of dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier\n",
    "clf_sub0 = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf_sub0.fit(X_train_sub0, y_train_sub0)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_RF_sub0 = clf_sub0.predict(X_test_sub0)\n",
    " \n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test_sub0, y_pred_RF_sub0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662936b",
   "metadata": {},
   "source": [
    "**Feature Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the feature scores\n",
    "\n",
    "feature_scores = pd.Series(clf_sub0.feature_importances_, index=X_train_sub0.columns).sort_values(ascending=False)\n",
    "\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e570c2",
   "metadata": {},
   "source": [
    "**ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_RF_sub0 = clf_sub0.predict_proba(X_test_sub0)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_RF_sub0)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba_RF_sub0)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce03a3",
   "metadata": {},
   "source": [
    "### Subsetting the dataset for TFIDF Comment feature values + Gender identities (Male, Female, Transgender, Other gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub1 = train_tfidf_data.iloc[:,26:]\n",
    "X_train_sub1[\"Male\"] = train_tfidf_data[\"Male\"]\n",
    "X_train_sub1[\"Female\"] = train_tfidf_data[\"Female\"]\n",
    "X_train_sub1[\"Transgender\"] = train_tfidf_data[\"Transgender\"]\n",
    "X_train_sub1[\"Other gender\"] = train_tfidf_data[\"Other gender\"]\n",
    "\n",
    "y_train_sub1 = train_tfidf_data['Toxicity']\n",
    "\n",
    "X_test_sub1 = dev_tfidf_data.iloc[:,26:]\n",
    "X_test_sub1[\"Male\"] = dev_tfidf_data[\"Male\"]\n",
    "X_test_sub1[\"Female\"] = dev_tfidf_data[\"Female\"]\n",
    "X_test_sub1[\"Transgender\"] = dev_tfidf_data[\"Transgender\"]\n",
    "X_test_sub1[\"Other gender\"] = dev_tfidf_data[\"Other gender\"]\n",
    "y_test_sub1 = dev_tfidf_data['Toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d442de4b",
   "metadata": {},
   "source": [
    "**Training Baseline Model with a subset of dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier\n",
    "clf_sub1 = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf_sub1.fit(X_train_sub1, y_train_sub1)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_RF_sub1 = clf_sub1.predict(X_test_sub1)\n",
    " \n",
    "# using metrics module for accuracy calculation\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test_sub1, y_pred_RF_sub1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cecff9",
   "metadata": {},
   "source": [
    "**Feature Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the feature scores\n",
    "\n",
    "feature_scores = pd.Series(clf_sub1.feature_importances_, index=X_train_sub1.columns).sort_values(ascending=False)\n",
    "\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148d6c4",
   "metadata": {},
   "source": [
    "#### Summarising Random Forest(Baseline), Logistic Regression and Naive Bayes accuracies for classifying toxicity in online comments on training with various subsets of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb599d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = PrettyTable()\n",
    "x1.field_names = [\"Model\", \"Accuracy\"]\n",
    "\n",
    "x1.add_row([\"Random Forest - Baseline(TFIDF values):\", 0.8243])\n",
    "x1.add_row([\"Random Forest - Baseline(Whole Dataset):\", 0.8249])\n",
    "x1.add_row([\"Random Forest - Baseline(TFIDF + Religion):\", 0.8237])\n",
    "x1.add_row([\"Random Forest - Baseline(TFIDF + Gender):\", 0.8246])\n",
    "x1.add_row([\"MLP Classifier (TFIDF values):\", 0.7880])\n",
    "x1.add_row([\"Multinomial Naive Bayes(TFIDF values):\", 0.8132])\n",
    "\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55f73b",
   "metadata": {},
   "source": [
    "**Hypotheses formulation:** </br>\n",
    "Three machine learning models are used namely Random Forest, Logistic Regression, and Multinomial Naive Bayes. Random Forest is used as baseline model to experiment with various subsets of dataset keeping other two machine learning models to only train with single subset of dataset. All three models are trained on TFIDF comment feature values and Random Forest performed well than others with a testing accuracy of 82.49% while MLP Classifier has 78.88% and Multinomial Naive Bayes has 81.32%.\n",
    "\n",
    "Additionally, few experiments conducted with baseline model for different subsets of datasets. Firstly the model trained with all identities and tfidf feature values which gave testing accuracy of 82.49%. Secondly the baseline model trained with tfidf feature values + religion identities and yielded testing accuracy of 82.37%. Lastly the baseline model trained with tfidf feature values + gender identities and yielded testing accuracy of 82.46%. The Baseline model performed well on all identities + tfidf feature values and also on only gender identity + tfidf values as shown in above accuracy table.\n",
    "\n",
    "\n",
    "**What are the strength and weaknessses of each machine learning paradigms?**</br>\n",
    "**Random Forest**</br>\n",
    "**Strengths:** Decision trees are capable of learning non-linear correlations and are somewhat resistant to outliers. In reality, ensembles perform well, winning several conventional (non-deep-learning) machine learning contests.</br>\n",
    "**Weaknesses:** Individual trees are prone to overfitting when left unconstrained because they might keep branching until they recall the training data. This can be mitigated by employing ensembles.\n",
    "\n",
    "**MLP Classifier**</br>\n",
    "**Strengths:** Multilayer Perceptrons have the advantage of learning non-linear models and the ability to train models in real-time (online learning).</br>\n",
    "**Weaknesses:**</br>\n",
    "MLP include too many parameters because it is fully connected.Each node is connected to another in a very dense web â€” resulting in redundancy and inefficiency.\n",
    "\n",
    "**Multinomial Naive Bayes**</br>\n",
    "**Strengths:** Even though the conditional independence assumption is rarely valid, NB models perform fairly well in practise, especially given their simplicity. They are simple to implement and may expand with your data.</br>\n",
    "**Weaknesses:** Because of their simplicity, NB models are frequently outperformed by models properly trained and tweaked utilising the preceding approaches.\n",
    "\n",
    "**How to adapt the models to close performance gap?**</br>\n",
    "You need to collect relevant data for training, and deploy pipelines that will feed data to the model when it is in production.\n",
    "\n",
    "**Which features are most predictive for the task?**</br>\n",
    "When training baseline model on all identities + tfidf feature values meaning complete dataset we find features that are more important in algorithm's decision making which are: Unnamed: 500, Unnamed: 28, Unnamed: 41, Unnamed: 86, Unnamed: 63,Unnamed: 735, White, Comment, Black, Christian, and homosexual gay or lesbian. When trained baseline model with a subset of dataset including tfidf values + gender identity the important features were: Unnamed: 500, Unnamed: 28, Unnamed: 41, Unnamed: 86, Unnamed: 63,Unnamed: 735, other gender.\n",
    "\n",
    "**Does method X outperform method Y on the task of comment toxicity because of theoretical property Z.**</br>\n",
    "The baseline model trained on complete dataset outperformed the baseline models trained on subsets of datasets because of all the features (all identities) were present in the complete dataset.\n",
    "\n",
    "**Ethical Concerns:**</br>\n",
    "If the model trained on biased data or on less data it will make an unjust or prejudicial distinction in the treatment of different categories of people, especially on the grounds of race, sex, age, or disability. To mitigate such event the algorithm must be feed with unbiased and complete data which represents every aspect of the community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf44cd",
   "metadata": {},
   "source": [
    "## Research Question 1: Does Unlabeled data improve comment toxicity classification?\n",
    "\n",
    "### Using Unsupervised Algorithms for Toxicity Classification in Online Comments\n",
    "\n",
    "### 1. DBSCAN clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e17443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X_train)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(YY, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(YY, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(YY, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(YY, labels))\n",
    "print(\n",
    "    \"Adjusted Mutual Information: %0.3f\"\n",
    "    % metrics.adjusted_mutual_info_score(YY, labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd1266",
   "metadata": {},
   "source": [
    "#### DBSCAN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823beeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Accuracy\n",
    "# y_true = dev_raw_data['Toxicity']\n",
    "# l = len(y_true)\n",
    "l = len(y_test)\n",
    "acc = sum([db.labels_[i]==y_test[i] for i in range(l)])/l\n",
    "print(\"Test Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6403919",
   "metadata": {},
   "source": [
    "### 2. Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training GMM\n",
    "# x = x.toarray()\n",
    "gm = GaussianMixture(n_components=2, random_state=0).fit(X_train)\n",
    "gm.means_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fd9e5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test.toarray()\n",
    "y_pred_gm = gm.predict(X_test)\n",
    "\n",
    "# Calculating Accuracy\n",
    "# y_true = dev_raw_data['Toxicity']\n",
    "l = len(y_test)\n",
    "acc = sum([y_pred_gm[i]==y_test[i] for i in range(l)])/l\n",
    "print(\"Test Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11010da",
   "metadata": {},
   "source": [
    "### 3. K-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training K-Means\n",
    "rng = RandomState(42)\n",
    "\n",
    "kmeans = KMeans(2, random_state=rng).fit(X_train)\n",
    "\n",
    "np.round(kmeans.cluster_centers_, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff493dc",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "#y_true = dev_raw_data['Toxicity']\n",
    "l = len(y_test)\n",
    "acc = sum([kmeans.labels_[i]==y_test[i] for i in range(l)])/l\n",
    "print(\"Test Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05072eb",
   "metadata": {},
   "source": [
    "### Answering Research Question#1:\n",
    "\n",
    "**The accuracy obtained from using unlabelled data to train unsupervised learning algorithms is far less than that of labelled data with supervised learning algorithms due to lot of variance in the distributaion of toxicity in different identities. Also a huge amount of sparse data is present in the dataset which makes it harder for unsupervised algorithms to draw a line, a common problem in machine learning is sparse data, which alters the performance of machine learning algorithms and their ability to calculate accurate predictions. The result is less accurate as we do not have any input data to train from. The model is learning from raw data without any prior knowledge. It is also a time-consuming process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"Accuracy\"]\n",
    "\n",
    "x.add_row([\"DBSCAN clustering algorithm:\", 0.0017])\n",
    "x.add_row([\"Gaussian Mixture Model:\", 0.3604])\n",
    "x.add_row([\"K-Means Clustering Algorithm:\", 0.7376])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4f3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
